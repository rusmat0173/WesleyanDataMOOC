{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working Doc, not Report Doc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commenced 20 Aug 2018"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps to do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 1. Import the two data files (predictors and targets) and merge, with pandas.\n",
    "> 2. Cut down to the predictors to be used. \n",
    "> 3. Create composite variables.\n",
    "> 4. Preliminary data exploration:\n",
    ">> Categorical variables: check number of categories including blanks; create filled bar charts.<br>\n",
    ">> Quantitative variables: get statistics such as mean, median, sd, range, # blanks; create violin or box plots.\n",
    "> 5. Decide on further data mgt., such as: \"rare\" categories, binned categories, delete all rows with blanks/if too many blanks.\n",
    "> 6. Train/test split.\n",
    "> 7. Create decision tree as a baseline reference for random forest.\n",
    "> 8. Create random forest. Tune parameters, consider boosting/bagging, confusion matrix, what is performance?\n",
    "> 9. Predict on test set.\n",
    "> 10. What do I now know? What does everything mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Import data files and merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data files\n",
    "predictors = pd.read_csv('/Users/RAhmed/data store/Wesleyan_Capstone/4910797b-ee55-40a7-8668-10efd5c1b960.csv')\n",
    "targets = pd.read_csv('/Users/RAhmed/data store/Wesleyan_Capstone/0bf8bc6e-30d0-4c50-956a-603fc693d966.csv')\n",
    "\n",
    "# check loaded properly\n",
    "predictors.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check loaded properly\n",
    "targets.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge predictors and targets, and check\n",
    "full_data = pd.merge(predictors, targets, on = 'id')\n",
    "full_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2A. Cut down to the predictors to be used "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will initially keep longitude and latitude, as will use these to find nearest neighbour for gps_height where that equals zero (i.e. not known/recorded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list columns to keep\n",
    "keep_cols = {'id', 'date_recorded', 'gps_height', 'installer', 'longitude', 'latitude', 'basin', 'region_code', \n",
    "             'population', 'public_meeting', 'scheme_management', 'permit', 'construction_year',\n",
    "             'extraction_type_group', 'management_group', \n",
    "             'payment_type', 'water_quality', 'quantity_group', 'source_type', \n",
    "             'source_class', 'waterpoint_type_group', 'status_group'}\n",
    "all_cols = set(full_data.head(0))\n",
    "# create columns to drop and check with a print\n",
    "drop_cols = all_cols - keep_cols\n",
    "print(drop_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create data set with just wanted variables\n",
    "select_data = full_data.drop(drop_cols, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "select_data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2B. Need to make most predictors into a categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = {'installer', 'basin', 'region_code', \n",
    "             'population', 'public_meeting', 'scheme_management', 'permit',\n",
    "             'extraction_type_group', 'management_group', 'payment_type', \n",
    "              'water_quality', 'quantity_group', 'source_type', \n",
    "             'source_class', 'waterpoint_type_group', 'status_group'}\n",
    "\n",
    "for item in categories:\n",
    "    select_data[item] = select_data[item].astype('category')\n",
    "# check types\n",
    "select_data.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3A. Create season_recorded, that takes category from month of recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initial testing\n",
    "for item in select_data['date_recorded'][3]:\n",
    "    print(item, type (item))\n",
    "select_data['date_recorded'][3][5] + select_data['date_recorded'][3][6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create seasons as per: https://www.tripsavvy.com/tanzania-weather-and-average-temperatures-4071465\n",
    "short_dry = {'01', '02'}\n",
    "long_dry= {'06', '07', '08', '09', '10'}\n",
    "short_rainy = {'11', '12'}\n",
    "long_rainy = {'03', '04', '05'}\n",
    "\n",
    "# create function to allocate seasons\n",
    "def season(month_str):\n",
    "    if month_str in short_dry:\n",
    "        return 'short_dry'\n",
    "    elif month_str in long_dry: \n",
    "        return 'long_dry'\n",
    "    elif month_str in short_rainy: \n",
    "        return 'short_rainy'\n",
    "    else:\n",
    "        return 'long_rainy'\n",
    "\n",
    "# create series for season based on date (= month) recorded, and check type is category\n",
    "df = select_data['date_recorded'].apply(lambda row: season(row[5] + row[6]))\n",
    "df = df.astype('category')\n",
    "df.dtypes\n",
    "\n",
    "# insert into select_data\n",
    "select_data.insert(2, 'season_recorded', df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check works\n",
    "select_data.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3B. Fill in missing gps_height variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approach will be to find nearest 3 neighbours and take average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing how loc and iloc work\n",
    "print(select_data.loc[0])\n",
    "print(select_data.loc[0].latitude) # <= this is key technique\n",
    "for row in select_data:\n",
    "    print(row)\n",
    "print(select_data.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to find key of maximum value in a dictionary\n",
    "# will be needed for the finding indices of dictionaries for nearest gps\n",
    "def find_dict_max(dict_):\n",
    "    for keys, values in dict_.items():\n",
    "        if values == max(dict_.values()):\n",
    "            return keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# will work on a smaller dataframe to make easier to follow. \n",
    "# need to make a deep copy, else original df also impacted.\n",
    "import copy\n",
    "df = copy.deepcopy(select_data[:20])\n",
    "\n",
    "# initially done as code, not a function\n",
    "for i in range(df['gps_height'].shape[0]):\n",
    "    if df['gps_height'][i] != 0: \n",
    "        pass\n",
    "    else:\n",
    "        neighbours = {-3:99999, -2:99999, -1:99999}\n",
    "        for j in range(df['gps_height'].shape[0]):\n",
    "            if j != i:\n",
    "                lat_i = df['latitude'][i]\n",
    "                lat_j = df['latitude'][j]\n",
    "                long_i = df['longitude'][i]\n",
    "                long_j = df['longitude'][j]\n",
    "                euclidian_dist = ((lat_i - lat_j)**2 + (long_i - long_j)**2)**0.5\n",
    "                # check if euclidian distance small enough to go into neighbours\n",
    "                if euclidian_dist < max(neighbours.values()):\n",
    "                    # if yes, need to remove max distance so far and return this distance\n",
    "                    neighbours.pop(find_max_key(neighbours))\n",
    "                    neighbours.update({j: euclidian_dist})\n",
    "                else:\n",
    "                    pass\n",
    "                \n",
    "        # now need to get average heights using indices in the neighbours keys\n",
    "        heights = []\n",
    "        for k in neighbours:\n",
    "            heights.append(df['gps_height'][k])\n",
    "        average_height = sum(heights)/len(heights)\n",
    "        df['gps_height'][i] = average_height  \n",
    "        \n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code works nicely. The solution is to have a function that you run several times to get rid of zeros.\n",
    "\n",
    "The issue you then run into is that applying the function on a row in the way we did with the season_recorded doesn't work. This is because you need to iterate over every row in any one row's function. This can be solved if you **apply the function to the whole dataframe,** as then you get iterate over i, j, etc.\n",
    "\n",
    "One complication as that (for some reason) could not change the df['gps_height'] in situ. Had to create another df for the result, and swap it out for the old one.\n",
    "\n",
    "Two points to note: i. For some height=0, their 3 nearest neighbours are also 0, so you still get zero entries; ii. Quite a number of longitudes are also zero, so also adjusted for that.  (i. can be solved by running function, e.g., twice.  N.B. As lat and long won't be used for predictive power, no point removing those zero rows if latitude = 0.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing\n",
    "df.loc[0]['gps_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing\n",
    "df['gps_height'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now create the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make as a function, so can be done twice\n",
    "def gps_helper(df):\n",
    "    \"\"\"\n",
    "    INPUT is the whole dataframe, so that you can easily traverse rows and columns\n",
    "    output is a single-column df tat has the new gps_heights\n",
    "    was hugely complex as (for some reason), could not directly change df's gps_height column\n",
    "    so you need to drop that column and put the new one in its place\n",
    "    OUTPUT is a single-column dataframe of new gps heights\n",
    "    IDEA: take 3 closest neighbours where gps_height is zero, and take average height of those.\n",
    "    That has to be added to the dataframe afterwards and seprately (having removed the old one with lots of zeros\n",
    "    \"\"\"\n",
    "    # Sadly, needed to create an output df that is added later, outside this function\n",
    "    list0 = [0] * df.shape[0]\n",
    "    df_gps = pd.DataFrame(list0, columns=['gps_height*'])\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.loc[i]['gps_height'] != 0:\n",
    "            df_gps.loc[i]['gps_height*'] = df.loc[i]['gps_height']\n",
    "            pass \n",
    "        else:\n",
    "            neighbours = {-3:99999, -2:99999, -1:99999}\n",
    "            for j in range(df.shape[0]):\n",
    "                # Many latitudes = 0, so also adjusting for this\n",
    "                if (j != i) and (df.loc[j]['latitude'] != 0):\n",
    "                    lat_i = df.loc[i]['latitude']\n",
    "                    lat_j = df.loc[j]['latitude']\n",
    "                    long_i = df.loc[i]['longitude']\n",
    "                    long_j = df.loc[j]['longitude']\n",
    "                    euclidian_dist = ((lat_i - lat_j)**2 + (long_i - long_j)**2)**0.5\n",
    "                    # check if euclidian distance small enough to go into neighbours\n",
    "                    if euclidian_dist < max(neighbours.values()):\n",
    "                        # if yes, need to remove max distance so far and return this distance\n",
    "                        neighbours.pop(find_max_key(neighbours))\n",
    "                        neighbours.update({j: euclidian_dist})\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            # now need to get average heights using indices in the neighbours keys\n",
    "            heights = []\n",
    "            for k in neighbours:\n",
    "                heights.append(df.loc[k]['gps_height'])\n",
    "                \n",
    "            average_height = sum(heights)/len(heights)\n",
    "            z = average_height\n",
    "            df_gps.loc[i]['gps_height*'] = z\n",
    "\n",
    "        \n",
    "    zero_count = 0\n",
    "    for m in range(df.shape[0]):\n",
    "        if df_gps.loc[m]['gps_height*'] == 0:\n",
    "            zero_count += 1\n",
    "    \n",
    "    print(\"number of zero heights is: {}\".format(zero_count))\n",
    "    return df_gps\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small copy of select_data on which to test and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "micro_data = copy.deepcopy(select_data[:30])\n",
    "temp = gps_helper(micro_data)\n",
    "temp\n",
    "# # drop old gps_height column and add new gps_height column (here temp)\n",
    "micro_data = micro_data.drop('gps_height', axis=1)\n",
    "# # Append a column to df\n",
    "micro_data.insert(3, 'gps_height', temp)\n",
    "micro_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do:** Run gps_helper on the complete data set that you want to use. (Potentially run it twice to get rid of remaining zero entries?) **N.B.** An issue is that if you remove rows afterwards as part of your data management, you won't be able to trace the heights back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check gps_height is correct type. 'int64' is correct.\n",
    "micro_data['gps_height'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3C. Fill-in missing construction_year by matching with similar observations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to fill in missing construction_year so that we can create an wpt_age variable. We can do this be taking a nearest neighbour approach, but simpler than Euclidian distance as only categorical variables.\n",
    "\n",
    "There are only a small number of chosen variables. So on your iterations through the df, if construction_year is non-zero and row matches on all chosen variables, its year is added to a list.  You would take an average of all the construction_years in the list and put that into the output dataframe. Rest of function and construction would be similar to gps_helper, above. \n",
    "\n",
    "Chosen categorical variables would be installer, scheme_management, extraction_type_group, and whether functional.  NOTE: the latter biases the \"target\" into the \"predictors\"!!! (Justification is that similar types would be similarly functional or not.)\n",
    "\n",
    "You could also do for lat/long. But I will leave, as well as adding complication, it is not clear that waterpumps would be installed temporally in geographical clusters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make as a function, so can be done twice\n",
    "def construction_year_helper(df):\n",
    "    \"\"\"\n",
    "    BASED ON gps_helper function\n",
    "    INPUT is the whole dataframe, so that you can easily traverse rows and columns\n",
    "    OUTPUT is a single-column df that has the new construction_year\n",
    "    IDEA: if construction_year is zero, iterate df to find matches for all chosen comparison variables.\n",
    "    If all match, add the iterative row's year to a list and take average of list at end. Put that into \n",
    "    output df's entry.\n",
    "    Is somewhat complex as cannot directly change df's construction_year values\n",
    "    \"\"\"\n",
    "    # Sadly, need to create an output df that is added later, outside this function\n",
    "    list0 = [0] * df.shape[0]\n",
    "    df_year = pd.DataFrame(list0, columns=['construction_year*'])\n",
    "    for i in range(df.shape[0]):\n",
    "        if df.loc[i]['construction_year'] != 0:\n",
    "            df_year.loc[i]['construction_year*'] = df.loc[i]['construction_year']\n",
    "            pass \n",
    "        else:\n",
    "            # initialise list that you will put matching years into\n",
    "            matching_years = []\n",
    "            for j in range(df.shape[0]):\n",
    "                # Many construction_year = 0, so not using these\n",
    "                if (j != i) and (df.loc[j]['construction_year'] != 0):\n",
    "                    # using a counter for readability\n",
    "                    counter = 0\n",
    "                    if df.loc[j]['installer'] == df.loc[j]['installer']:\n",
    "                        counter += 1\n",
    "                    if df.loc[j]['scheme_management'] == df.loc[j]['scheme_management']:\n",
    "                        counter += 1   \n",
    "                    if df.loc[j]['extraction_type_group'] == df.loc[j]['extraction_type_group']:\n",
    "                        counter += 1 \n",
    "                    if df.loc[j]['status_group'] == df.loc[j]['status_group']:\n",
    "                        counter += 1     \n",
    "                    # check if all items match\n",
    "                    if counter == 4:\n",
    "                        # if yes, add to matching_years\n",
    "                        matching_years.append(df.loc[j]['construction_year'])\n",
    "                    else:\n",
    "                        pass\n",
    "\n",
    "            # now need to get average year of the matches   \n",
    "            average_year = sum(matching_years)/len(matching_years)\n",
    "            z = round(average_year)\n",
    "            df_year.loc[i]['construction_year*'] = z\n",
    "\n",
    "        \n",
    "    zero_count = 0\n",
    "    for m in range(df.shape[0]):\n",
    "        if df_year.loc[m]['construction_year*'] == 0:\n",
    "            zero_count += 1\n",
    "    \n",
    "    print(\"number of zero years is: {}\".format(zero_count))\n",
    "    return df_year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a small copy of select_data on which to test and check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data = copy.deepcopy(select_data[:30])\n",
    "temp = construction_year_helper(mini_data)\n",
    "temp\n",
    "# # drop old construction_year column and add new construction_year column (here temp)\n",
    "mini_data = mini_data.drop('construction_year', axis=1)\n",
    "# # Append a column to df\n",
    "mini_data.insert(13, 'construction_year', temp)\n",
    "mini_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do:** Run construction_year_helper on the complete data set that you want to use. No need to run twice as seems to have no zero entries. (Shouldn't do by its construction.) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3D. wpt_age variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(select_data['date_recorded'][1]))\n",
    "print(type(select_data['construction_year'][1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A significant issue was that a lot of missing contruction_year. That is now solved by the construction_year_helper, above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now need to change year recorded into a numerical value from a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# just testing\n",
    "x = '2013'\n",
    "y = int(x)\n",
    "print(y, type(y))\n",
    "z = float(x)\n",
    "print(z, type(z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create helper function and test\n",
    "def year_convert(date_recorded):\n",
    "    \"\"\"\n",
    "    takes custom string (e.g. '2013-04-17') and converts to a float\n",
    "    is not super accurate as assumes all months have 31 days\n",
    "    \n",
    "    \"\"\"\n",
    "    year = float(date_recorded[:4])\n",
    "    month = float(date_recorded[5:7])\n",
    "    day = float(date_recorded[8:])\n",
    "    \n",
    "    return year + ((month - 1) / 12) + (day / (31*12)) \n",
    "\n",
    "year_convert('2013-02-28')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a separate new df called wpt_age, and then add to global df.\n",
    "\n",
    "Test on a smaller data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_wpt_age = mini_data['date_recorded'].apply(year_convert) - mini_data['construction_year']\n",
    "df_wpt_age.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now add to global dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.insert(14, 'wpt_age', df_wpt_age)\n",
    "mini_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**To Do:** Run year_convert on the complete data set that you want to use. Add the single-column df to the global dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3E. Applying improvements to global data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will use select_data as basis, making a copy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_data = copy.deepcopy(select_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "season_recorded was a simple fix, so that has already been globally implemented. Next is to fix the missing gps_height values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gps_temp = gps_helper(stage3_data)\n",
    "# # drop old gps_height column and add new gps_height column (here temp)\n",
    "stage3_data = stage3_data.drop('gps_height', axis=1)\n",
    "# # Append a column to df\n",
    "stage3_data.insert(3, 'gps_height', gps_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stage3_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is taking a very long time, could consider sorting by latitude first.  Could then just check the 3,000-4,000 either side?  (Thinking about this, I've asked it to do 59,000\\**2 checks, = nearly 3.5 billion checks against other variables.\n",
    "\n",
    "For missing construction_year, could first sort by installer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Preliminary data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Will have to fix NaNs in various columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.loc[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(mini_data.index[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mini_data.loc[1]['gps_height']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
